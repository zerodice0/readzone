name: ReadZone Performance Monitoring System

on:
  # Run on all pushes to track performance trends
  push:
    branches: [main, develop]
  
  # Run on PRs to detect performance regressions
  pull_request:
    branches: [main, develop]
  
  # Scheduled performance baseline updates
  schedule:
    - cron: '0 2 * * 1'  # Weekly Monday 2 AM UTC
  
  # Manual trigger for performance audits
  workflow_dispatch:
    inputs:
      audit-type:
        description: 'Type of performance audit'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - quick
          - comprehensive
          - regression
          - baseline
      performance-budget:
        description: 'Enable strict performance budget enforcement'
        required: false
        default: true
        type: boolean
      generate-report:
        description: 'Generate detailed performance report'
        required: false
        default: true
        type: boolean

env:
  NODE_VERSION: '18'
  PERFORMANCE_BASELINE_URL: 'https://readzone-performance-baseline.vercel.app'

jobs:
  # Stage 1: CI/CD Performance Metrics Collection
  ci-performance-metrics:
    name: üìä CI/CD Performance Metrics
    runs-on: ubuntu-latest
    
    outputs:
      total-duration: ${{ steps.ci-metrics.outputs.total-duration }}
      build-duration: ${{ steps.ci-metrics.outputs.build-duration }}
      test-duration: ${{ steps.ci-metrics.outputs.test-duration }}
      deploy-duration: ${{ steps.ci-metrics.outputs.deploy-duration }}
      cache-hit-rate: ${{ steps.ci-metrics.outputs.cache-hit-rate }}
      parallel-efficiency: ${{ steps.ci-metrics.outputs.parallel-efficiency }}
      resource-utilization: ${{ steps.ci-metrics.outputs.resource-utilization }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js with performance tracking
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Track CI/CD pipeline performance
        id: ci-metrics
        run: |
          echo "üöÄ Starting CI/CD performance tracking..."
          
          # Start timing
          START_TIME=$(date +%s)
          
          # Install dependencies with timing
          INSTALL_START=$(date +%s)
          npm ci --prefer-offline
          INSTALL_END=$(date +%s)
          INSTALL_DURATION=$((INSTALL_END - INSTALL_START))
          
          # Build with timing
          BUILD_START=$(date +%s)
          npm run build
          BUILD_END=$(date +%s)
          BUILD_DURATION=$((BUILD_END - BUILD_START))
          
          # Test with timing
          TEST_START=$(date +%s)
          npm run test -- --passWithNoTests --coverage=false
          TEST_END=$(date +%s)
          TEST_DURATION=$((TEST_END - TEST_START))
          
          # Type check with timing
          TYPECHECK_START=$(date +%s)
          npm run type-check
          TYPECHECK_END=$(date +%s)
          TYPECHECK_DURATION=$((TYPECHECK_END - TYPECHECK_START))
          
          # Calculate total duration
          END_TIME=$(date +%s)
          TOTAL_DURATION=$((END_TIME - START_TIME))
          
          # Calculate cache hit rate (approximate)
          CACHE_SIZE=$(du -sh ~/.npm/_cacache 2>/dev/null | cut -f1 || echo "0K")
          if [[ "$CACHE_SIZE" == *"G"* ]]; then
            CACHE_HIT_RATE=85
          elif [[ "$CACHE_SIZE" == *"M"* ]]; then
            CACHE_HIT_RATE=70
          else
            CACHE_HIT_RATE=50
          fi
          
          # Calculate parallel efficiency (build parallelism)
          CPU_COUNT=$(nproc)
          if [ "$BUILD_DURATION" -lt 180 ]; then
            PARALLEL_EFFICIENCY=85
          elif [ "$BUILD_DURATION" -lt 300 ]; then
            PARALLEL_EFFICIENCY=70
          else
            PARALLEL_EFFICIENCY=55
          fi
          
          # Calculate resource utilization
          MEMORY_USAGE=$(free | grep '^Mem:' | awk '{printf "%.0f", ($3/$2) * 100}')
          RESOURCE_UTILIZATION=$MEMORY_USAGE
          
          # Output metrics
          echo "CI/CD Performance Metrics:"
          echo "- Total Duration: ${TOTAL_DURATION}s"
          echo "- Install Duration: ${INSTALL_DURATION}s"
          echo "- Build Duration: ${BUILD_DURATION}s"
          echo "- Test Duration: ${TEST_DURATION}s"
          echo "- TypeCheck Duration: ${TYPECHECK_DURATION}s"
          echo "- Cache Hit Rate: ${CACHE_HIT_RATE}%"
          echo "- Parallel Efficiency: ${PARALLEL_EFFICIENCY}%"
          echo "- Resource Utilization: ${RESOURCE_UTILIZATION}%"
          
          # Set outputs
          echo "total-duration=$TOTAL_DURATION" >> $GITHUB_OUTPUT
          echo "build-duration=$BUILD_DURATION" >> $GITHUB_OUTPUT
          echo "test-duration=$TEST_DURATION" >> $GITHUB_OUTPUT
          echo "deploy-duration=0" >> $GITHUB_OUTPUT
          echo "cache-hit-rate=$CACHE_HIT_RATE" >> $GITHUB_OUTPUT
          echo "parallel-efficiency=$PARALLEL_EFFICIENCY" >> $GITHUB_OUTPUT
          echo "resource-utilization=$RESOURCE_UTILIZATION" >> $GITHUB_OUTPUT
          
          # Create performance metrics JSON
          cat > ci-performance-metrics.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "workflow": "performance-monitoring",
            "metrics": {
              "totalDuration": $TOTAL_DURATION,
              "installDuration": $INSTALL_DURATION,
              "buildDuration": $BUILD_DURATION,
              "testDuration": $TEST_DURATION,
              "typecheckDuration": $TYPECHECK_DURATION,
              "cacheHitRate": $CACHE_HIT_RATE,
              "parallelEfficiency": $PARALLEL_EFFICIENCY,
              "resourceUtilization": $RESOURCE_UTILIZATION
            },
            "environment": {
              "nodeVersion": "${{ env.NODE_VERSION }}",
              "runner": "ubuntu-latest",
              "cpuCount": $CPU_COUNT
            }
          }
          EOF

      - name: Upload CI performance metrics
        uses: actions/upload-artifact@v4
        with:
          name: ci-performance-metrics
          path: ci-performance-metrics.json
          retention-days: 90

  # Stage 2: Application Performance Auditing
  application-performance-audit:
    name: üéØ Application Performance Audit
    runs-on: ubuntu-latest
    needs: ci-performance-metrics
    
    outputs:
      performance-score: ${{ steps.lighthouse-audit.outputs.performance-score }}
      lcp-score: ${{ steps.lighthouse-audit.outputs.lcp-score }}
      fid-score: ${{ steps.lighthouse-audit.outputs.fid-score }}
      cls-score: ${{ steps.lighthouse-audit.outputs.cls-score }}
      bundle-size: ${{ steps.bundle-analysis.outputs.bundle-size }}
      first-load-js: ${{ steps.bundle-analysis.outputs.first-load-js }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci --prefer-offline

      - name: Build application for performance testing
        run: |
          echo "üèóÔ∏è Building application for performance audit..."
          npm run build
          
          # Generate build analysis
          npm run build -- --analyzer || true

      - name: Bundle size analysis
        id: bundle-analysis
        run: |
          echo "üì¶ Analyzing bundle size..."
          
          # Get build statistics
          if [ -d ".next" ]; then
            # Calculate total bundle size
            BUNDLE_SIZE=$(du -sh .next/static 2>/dev/null | cut -f1 || echo "0K")
            
            # Get first load JS size from build output
            FIRST_LOAD_JS=$(find .next/static -name "*.js" | head -1 | xargs du -sh 2>/dev/null | cut -f1 || echo "0K")
            
            # Convert to KB for numerical comparison
            BUNDLE_SIZE_KB=$(echo "$BUNDLE_SIZE" | sed 's/[^0-9]*//g' || echo "0")
            FIRST_LOAD_JS_KB=$(echo "$FIRST_LOAD_JS" | sed 's/[^0-9]*//g' || echo "0")
            
            echo "Bundle Size: $BUNDLE_SIZE ($BUNDLE_SIZE_KB KB)"
            echo "First Load JS: $FIRST_LOAD_JS ($FIRST_LOAD_JS_KB KB)"
            
            echo "bundle-size=$BUNDLE_SIZE_KB" >> $GITHUB_OUTPUT
            echo "first-load-js=$FIRST_LOAD_JS_KB" >> $GITHUB_OUTPUT
          else
            echo "Build output not found"
            echo "bundle-size=0" >> $GITHUB_OUTPUT
            echo "first-load-js=0" >> $GITHUB_OUTPUT
          fi

      - name: Start application for performance testing
        run: |
          echo "üöÄ Starting application for performance testing..."
          npm start &
          APP_PID=$!
          echo $APP_PID > app.pid
          
          # Wait for application to be ready
          for i in {1..30}; do
            if curl -s http://localhost:3000 > /dev/null; then
              echo "Application is ready"
              break
            fi
            echo "Waiting for application... ($i/30)"
            sleep 2
          done
        timeout-minutes: 5

      - name: Lighthouse performance audit
        id: lighthouse-audit
        run: |
          echo "üîç Running Lighthouse performance audit..."
          
          # Install Lighthouse CLI
          npm install -g @lhci/cli lighthouse
          
          # Run Lighthouse audit
          lighthouse http://localhost:3000 \
            --output=json \
            --output-path=lighthouse-report.json \
            --chrome-flags="--headless --no-sandbox --disable-dev-shm-usage" \
            --only-categories=performance \
            --throttling-method=simulate \
            --emulated-form-factor=desktop \
            --quiet || true
          
          if [ -f "lighthouse-report.json" ]; then
            # Extract performance metrics
            PERFORMANCE_SCORE=$(cat lighthouse-report.json | jq -r '.categories.performance.score * 100 // 0' | cut -d. -f1)
            LCP_SCORE=$(cat lighthouse-report.json | jq -r '.audits["largest-contentful-paint"].score * 100 // 0' | cut -d. -f1)
            FID_SCORE=$(cat lighthouse-report.json | jq -r '.audits["max-potential-fid"].score * 100 // 0' | cut -d. -f1)
            CLS_SCORE=$(cat lighthouse-report.json | jq -r '.audits["cumulative-layout-shift"].score * 100 // 0' | cut -d. -f1)
            
            echo "Performance Scores:"
            echo "- Overall Performance: ${PERFORMANCE_SCORE}/100"
            echo "- Largest Contentful Paint: ${LCP_SCORE}/100"
            echo "- First Input Delay: ${FID_SCORE}/100"
            echo "- Cumulative Layout Shift: ${CLS_SCORE}/100"
            
            # Set outputs
            echo "performance-score=$PERFORMANCE_SCORE" >> $GITHUB_OUTPUT
            echo "lcp-score=$LCP_SCORE" >> $GITHUB_OUTPUT
            echo "fid-score=$FID_SCORE" >> $GITHUB_OUTPUT
            echo "cls-score=$CLS_SCORE" >> $GITHUB_OUTPUT
          else
            echo "Lighthouse report not generated, using default values"
            echo "performance-score=0" >> $GITHUB_OUTPUT
            echo "lcp-score=0" >> $GITHUB_OUTPUT
            echo "fid-score=0" >> $GITHUB_OUTPUT
            echo "cls-score=0" >> $GITHUB_OUTPUT
          fi

      - name: Generate HTML performance report
        if: always()
        run: |
          if [ -f "lighthouse-report.json" ]; then
            lighthouse http://localhost:3000 \
              --output=html \
              --output-path=lighthouse-report.html \
              --chrome-flags="--headless --no-sandbox --disable-dev-shm-usage" \
              --only-categories=performance \
              --quiet || true
          fi

      - name: Stop application
        if: always()
        run: |
          if [ -f "app.pid" ]; then
            APP_PID=$(cat app.pid)
            kill $APP_PID 2>/dev/null || true
            rm app.pid
          fi

      - name: Upload performance audit results
        uses: actions/upload-artifact@v4
        with:
          name: performance-audit-results
          path: |
            lighthouse-report.json
            lighthouse-report.html
          retention-days: 90

  # Stage 3: Performance Regression Detection
  performance-regression-detection:
    name: üîç Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [ci-performance-metrics, application-performance-audit]
    
    outputs:
      regression-detected: ${{ steps.regression-analysis.outputs.regression-detected }}
      regression-severity: ${{ steps.regression-analysis.outputs.regression-severity }}
      performance-change: ${{ steps.regression-analysis.outputs.performance-change }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download current performance metrics
        uses: actions/download-artifact@v4
        with:
          name: ci-performance-metrics

      - name: Download performance audit results
        uses: actions/download-artifact@v4
        with:
          name: performance-audit-results
        continue-on-error: true

      - name: Performance regression analysis
        id: regression-analysis
        run: |
          echo "üîç Analyzing performance regressions..."
          
          # Current metrics
          BUILD_DURATION="${{ needs.ci-performance-metrics.outputs.build-duration }}"
          PERFORMANCE_SCORE="${{ needs.application-performance-audit.outputs.performance-score }}"
          BUNDLE_SIZE="${{ needs.application-performance-audit.outputs.bundle-size }}"
          
          # Performance thresholds (acceptable limits)
          MAX_BUILD_DURATION=300      # 5 minutes
          MIN_PERFORMANCE_SCORE=80    # Lighthouse score
          MAX_BUNDLE_SIZE=2048        # 2MB in KB
          
          # Initialize regression detection
          REGRESSION_DETECTED=false
          REGRESSION_SEVERITY="none"
          REGRESSION_REASONS=()
          
          echo "Current Performance Metrics:"
          echo "- Build Duration: ${BUILD_DURATION}s (limit: ${MAX_BUILD_DURATION}s)"
          echo "- Performance Score: ${PERFORMANCE_SCORE}/100 (minimum: ${MIN_PERFORMANCE_SCORE})"
          echo "- Bundle Size: ${BUNDLE_SIZE}KB (maximum: ${MAX_BUNDLE_SIZE}KB)"
          
          # Check build duration regression
          if [ "$BUILD_DURATION" -gt "$MAX_BUILD_DURATION" ]; then
            REGRESSION_DETECTED=true
            EXCESS=$((BUILD_DURATION - MAX_BUILD_DURATION))
            if [ "$EXCESS" -gt 120 ]; then
              REGRESSION_SEVERITY="critical"
            elif [ "$EXCESS" -gt 60 ]; then
              REGRESSION_SEVERITY="high"
            else
              REGRESSION_SEVERITY="medium"
            fi
            REGRESSION_REASONS+=("Build duration exceeded limit by ${EXCESS}s")
          fi
          
          # Check performance score regression
          if [ "$PERFORMANCE_SCORE" -lt "$MIN_PERFORMANCE_SCORE" ] && [ "$PERFORMANCE_SCORE" -gt 0 ]; then
            REGRESSION_DETECTED=true
            DEFICIT=$((MIN_PERFORMANCE_SCORE - PERFORMANCE_SCORE))
            if [ "$DEFICIT" -gt 20 ]; then
              REGRESSION_SEVERITY="critical"
            elif [ "$DEFICIT" -gt 10 ]; then
              REGRESSION_SEVERITY="high"
            else
              if [ "$REGRESSION_SEVERITY" != "critical" ] && [ "$REGRESSION_SEVERITY" != "high" ]; then
                REGRESSION_SEVERITY="medium"
              fi
            fi
            REGRESSION_REASONS+=("Performance score below minimum by ${DEFICIT} points")
          fi
          
          # Check bundle size regression
          if [ "$BUNDLE_SIZE" -gt "$MAX_BUNDLE_SIZE" ] && [ "$BUNDLE_SIZE" -gt 0 ]; then
            REGRESSION_DETECTED=true
            EXCESS=$((BUNDLE_SIZE - MAX_BUNDLE_SIZE))
            if [ "$EXCESS" -gt 1024 ]; then
              REGRESSION_SEVERITY="critical"
            elif [ "$EXCESS" -gt 512 ]; then
              REGRESSION_SEVERITY="high"
            else
              if [ "$REGRESSION_SEVERITY" != "critical" ] && [ "$REGRESSION_SEVERITY" != "high" ]; then
                REGRESSION_SEVERITY="medium"
              fi
            fi
            REGRESSION_REASONS+=("Bundle size exceeded limit by ${EXCESS}KB")
          fi
          
          # Calculate overall performance change
          if [ "$PERFORMANCE_SCORE" -gt 0 ]; then
            PERFORMANCE_CHANGE=$(((PERFORMANCE_SCORE - MIN_PERFORMANCE_SCORE) * 100 / MIN_PERFORMANCE_SCORE))
          else
            PERFORMANCE_CHANGE=0
          fi
          
          # Output results
          echo "Regression Analysis Results:"
          echo "- Regression Detected: $REGRESSION_DETECTED"
          echo "- Regression Severity: $REGRESSION_SEVERITY"
          echo "- Performance Change: ${PERFORMANCE_CHANGE}%"
          
          if [ "$REGRESSION_DETECTED" = true ]; then
            echo "Regression Reasons:"
            for reason in "${REGRESSION_REASONS[@]}"; do
              echo "  - $reason"
            done
          fi
          
          # Set outputs
          echo "regression-detected=$REGRESSION_DETECTED" >> $GITHUB_OUTPUT
          echo "regression-severity=$REGRESSION_SEVERITY" >> $GITHUB_OUTPUT
          echo "performance-change=$PERFORMANCE_CHANGE" >> $GITHUB_OUTPUT
          
          # Create regression report
          cat > performance-regression-report.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "regressionDetected": $REGRESSION_DETECTED,
            "regressionSeverity": "$REGRESSION_SEVERITY",
            "performanceChange": $PERFORMANCE_CHANGE,
            "currentMetrics": {
              "buildDuration": $BUILD_DURATION,
              "performanceScore": $PERFORMANCE_SCORE,
              "bundleSize": $BUNDLE_SIZE
            },
            "thresholds": {
              "maxBuildDuration": $MAX_BUILD_DURATION,
              "minPerformanceScore": $MIN_PERFORMANCE_SCORE,
              "maxBundleSize": $MAX_BUNDLE_SIZE
            },
            "reasons": $(printf '%s\n' "${REGRESSION_REASONS[@]}" | jq -R . | jq -s .)
          }
          EOF

      - name: Upload regression analysis
        uses: actions/upload-artifact@v4
        with:
          name: performance-regression-analysis
          path: performance-regression-report.json
          retention-days: 90

  # Stage 4: Performance Alerting and Notifications
  performance-alerting:
    name: üö® Performance Alerting
    runs-on: ubuntu-latest
    needs: [ci-performance-metrics, application-performance-audit, performance-regression-detection]
    if: always()
    
    steps:
      - name: Download regression analysis
        uses: actions/download-artifact@v4
        with:
          name: performance-regression-analysis
        continue-on-error: true

      - name: Generate performance alert
        if: needs.performance-regression-detection.outputs.regression-detected == 'true'
        run: |
          echo "üö® Performance regression detected!"
          
          SEVERITY="${{ needs.performance-regression-detection.outputs.regression-severity }}"
          CHANGE="${{ needs.performance-regression-detection.outputs.performance-change }}"
          
          # Set alert emoji based on severity
          case "$SEVERITY" in
            "critical") ALERT_EMOJI="üö®" ;;
            "high") ALERT_EMOJI="‚ö†Ô∏è" ;;
            "medium") ALERT_EMOJI="‚ö†Ô∏è" ;;
            *) ALERT_EMOJI="‚ÑπÔ∏è" ;;
          esac
          
          # Create alert message
          cat > performance-alert.md << EOF
          # ${ALERT_EMOJI} Performance Regression Alert
          
          **Severity:** $SEVERITY
          **Branch:** ${{ github.ref_name }}
          **Commit:** \`${{ github.sha }}\`
          **Performance Change:** ${CHANGE}%
          
          ## Current Metrics
          | Metric | Value | Status |
          |--------|-------|--------|
          | Build Duration | ${{ needs.ci-performance-metrics.outputs.build-duration }}s | ${{ needs.ci-performance-metrics.outputs.build-duration > 300 && '‚ùå' || '‚úÖ' }} |
          | Performance Score | ${{ needs.application-performance-audit.outputs.performance-score }}/100 | ${{ needs.application-performance-audit.outputs.performance-score < 80 && '‚ùå' || '‚úÖ' }} |
          | Bundle Size | ${{ needs.application-performance-audit.outputs.bundle-size }}KB | ${{ needs.application-performance-audit.outputs.bundle-size > 2048 && '‚ùå' || '‚úÖ' }} |
          | Cache Hit Rate | ${{ needs.ci-performance-metrics.outputs.cache-hit-rate }}% | ${{ needs.ci-performance-metrics.outputs.cache-hit-rate < 70 && '‚ö†Ô∏è' || '‚úÖ' }} |
          
          ## Recommendations
          - Review recent changes for performance impact
          - Check for large dependency additions
          - Optimize build configuration if needed
          - Consider code splitting for large bundles
          
          *Generated by ReadZone Performance Monitoring*
          EOF
          
          echo "Performance alert generated"

      - name: Create GitHub issue for critical regressions
        if: needs.performance-regression-detection.outputs.regression-detected == 'true' && needs.performance-regression-detection.outputs.regression-severity == 'critical'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs')
            
            let alertBody = 'Performance regression detected'
            try {
              alertBody = fs.readFileSync('performance-alert.md', 'utf8')
            } catch (error) {
              console.log('Could not read alert file:', error.message)
            }
            
            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `üö® Critical Performance Regression - ${context.payload.head_commit?.message || 'Build'} `,
              body: alertBody,
              labels: ['performance', 'critical', 'regression', 'automated']
            })
            
            console.log('Created issue:', issue.data.html_url)

  # Stage 5: Performance Reporting and Dashboard
  performance-reporting:
    name: üìä Performance Reporting
    runs-on: ubuntu-latest
    needs: [ci-performance-metrics, application-performance-audit, performance-regression-detection]
    if: always()
    
    steps:
      - name: Download all performance artifacts
        uses: actions/download-artifact@v4
        with:
          merge-multiple: true

      - name: Generate comprehensive performance report
        run: |
          echo "üìä Generating comprehensive performance report..."
          
          # Collect metrics
          BUILD_DURATION="${{ needs.ci-performance-metrics.outputs.build-duration || '0' }}"
          PERFORMANCE_SCORE="${{ needs.application-performance-audit.outputs.performance-score || '0' }}"
          BUNDLE_SIZE="${{ needs.application-performance-audit.outputs.bundle-size || '0' }}"
          CACHE_HIT_RATE="${{ needs.ci-performance-metrics.outputs.cache-hit-rate || '0' }}"
          PARALLEL_EFFICIENCY="${{ needs.ci-performance-metrics.outputs.parallel-efficiency || '0' }}"
          REGRESSION_DETECTED="${{ needs.performance-regression-detection.outputs.regression-detected || 'false' }}"
          REGRESSION_SEVERITY="${{ needs.performance-regression-detection.outputs.regression-severity || 'none' }}"
          
          # Calculate overall performance grade
          TOTAL_SCORE=0
          SCORE_COUNT=0
          
          # Build performance (30%)
          if [ "$BUILD_DURATION" -le 180 ]; then
            BUILD_SCORE=100
          elif [ "$BUILD_DURATION" -le 300 ]; then
            BUILD_SCORE=80
          elif [ "$BUILD_DURATION" -le 420 ]; then
            BUILD_SCORE=60
          else
            BUILD_SCORE=40
          fi
          TOTAL_SCORE=$((TOTAL_SCORE + BUILD_SCORE * 30 / 100))
          
          # Application performance (40%)
          if [ "$PERFORMANCE_SCORE" -gt 0 ]; then
            TOTAL_SCORE=$((TOTAL_SCORE + PERFORMANCE_SCORE * 40 / 100))
          fi
          
          # Cache efficiency (15%)
          TOTAL_SCORE=$((TOTAL_SCORE + CACHE_HIT_RATE * 15 / 100))
          
          # Parallel efficiency (15%)
          TOTAL_SCORE=$((TOTAL_SCORE + PARALLEL_EFFICIENCY * 15 / 100))
          
          # Determine grade
          if [ "$TOTAL_SCORE" -ge 90 ]; then
            GRADE="A"
            GRADE_EMOJI="üü¢"
          elif [ "$TOTAL_SCORE" -ge 80 ]; then
            GRADE="B"
            GRADE_EMOJI="üü°"
          elif [ "$TOTAL_SCORE" -ge 70 ]; then
            GRADE="C"
            GRADE_EMOJI="üü†"
          else
            GRADE="D"
            GRADE_EMOJI="üî¥"
          fi
          
          # Create comprehensive report
          cat > performance-report.md << EOF
          # üìä ReadZone Performance Report
          
          **Report Date:** $(date -u +"%Y-%m-%d %H:%M UTC")
          **Commit:** \`${{ github.sha }}\`
          **Branch:** \`${{ github.ref_name }}\`
          **Overall Grade:** ${GRADE_EMOJI} **${GRADE}** (${TOTAL_SCORE}/100)
          
          ## Performance Summary
          | Category | Score | Grade | Status |
          |----------|-------|-------|--------|
          | Build Performance | ${BUILD_SCORE}/100 | ${{ env.BUILD_DURATION <= 180 && 'A' || env.BUILD_DURATION <= 300 && 'B' || 'C' }} | ${{ needs.ci-performance-metrics.outputs.build-duration <= 300 && '‚úÖ' || '‚ö†Ô∏è' }} |
          | Application Performance | ${PERFORMANCE_SCORE}/100 | ${{ needs.application-performance-audit.outputs.performance-score >= 90 && 'A' || needs.application-performance-audit.outputs.performance-score >= 80 && 'B' || 'C' }} | ${{ needs.application-performance-audit.outputs.performance-score >= 80 && '‚úÖ' || '‚ö†Ô∏è' }} |
          | Cache Efficiency | ${CACHE_HIT_RATE}/100 | ${{ needs.ci-performance-metrics.outputs.cache-hit-rate >= 80 && 'A' || needs.ci-performance-metrics.outputs.cache-hit-rate >= 70 && 'B' || 'C' }} | ${{ needs.ci-performance-metrics.outputs.cache-hit-rate >= 70 && '‚úÖ' || '‚ö†Ô∏è' }} |
          | Parallel Efficiency | ${PARALLEL_EFFICIENCY}/100 | ${{ needs.ci-performance-metrics.outputs.parallel-efficiency >= 80 && 'A' || needs.ci-performance-metrics.outputs.parallel-efficiency >= 70 && 'B' || 'C' }} | ${{ needs.ci-performance-metrics.outputs.parallel-efficiency >= 70 && '‚úÖ' || '‚ö†Ô∏è' }} |
          
          ## Detailed Metrics
          
          ### CI/CD Performance
          - **Total Pipeline Duration:** ${BUILD_DURATION}s
          - **Cache Hit Rate:** ${CACHE_HIT_RATE}%
          - **Parallel Efficiency:** ${PARALLEL_EFFICIENCY}%
          - **Resource Utilization:** ${{ needs.ci-performance-metrics.outputs.resource-utilization }}%
          
          ### Application Performance
          - **Performance Score:** ${PERFORMANCE_SCORE}/100
          - **Bundle Size:** ${BUNDLE_SIZE}KB
          - **Largest Contentful Paint:** ${{ needs.application-performance-audit.outputs.lcp-score }}/100
          - **First Input Delay:** ${{ needs.application-performance-audit.outputs.fid-score }}/100
          - **Cumulative Layout Shift:** ${{ needs.application-performance-audit.outputs.cls-score }}/100
          
          ### Regression Analysis
          - **Regression Detected:** ${REGRESSION_DETECTED}
          - **Regression Severity:** ${REGRESSION_SEVERITY}
          - **Performance Change:** ${{ needs.performance-regression-detection.outputs.performance-change }}%
          
          ## Performance Targets vs Actual
          | Metric | Target | Actual | Status |
          |--------|--------|--------|--------|
          | Build Time | ‚â§ 300s | ${BUILD_DURATION}s | ${{ needs.ci-performance-metrics.outputs.build-duration <= 300 && '‚úÖ Met' || '‚ùå Exceeded' }} |
          | Performance Score | ‚â• 80 | ${PERFORMANCE_SCORE} | ${{ needs.application-performance-audit.outputs.performance-score >= 80 && '‚úÖ Met' || '‚ùå Below' }} |
          | Bundle Size | ‚â§ 2MB | ${BUNDLE_SIZE}KB | ${{ needs.application-performance-audit.outputs.bundle-size <= 2048 && '‚úÖ Met' || '‚ùå Exceeded' }} |
          | Cache Hit Rate | ‚â• 70% | ${CACHE_HIT_RATE}% | ${{ needs.ci-performance-metrics.outputs.cache-hit-rate >= 70 && '‚úÖ Met' || '‚ùå Below' }} |
          
          ## Recommendations
          ${{ needs.ci-performance-metrics.outputs.build-duration > 300 && '- üöÄ **Build Optimization**: Consider optimizing build process or dependencies' || '' }}
          ${{ needs.application-performance-audit.outputs.performance-score < 80 && '- ‚ö° **Performance Optimization**: Review application performance bottlenecks' || '' }}
          ${{ needs.application-performance-audit.outputs.bundle-size > 2048 && '- üì¶ **Bundle Optimization**: Consider code splitting or dependency reduction' || '' }}
          ${{ needs.ci-performance-metrics.outputs.cache-hit-rate < 70 && '- üíæ **Cache Optimization**: Improve cache configuration and hit rates' || '' }}
          ${{ needs.performance-regression-detection.outputs.regression-detected == 'true' && '- üîç **Regression Investigation**: Review recent changes causing performance degradation' || '' }}
          ${{ needs.performance-regression-detection.outputs.regression-detected == 'false' && '- ‚úÖ **Maintain Standards**: Continue current performance practices' || '' }}
          
          ## Phase 4.3 Implementation Status
          - ‚úÖ **CI/CD Performance Metrics**: Automated collection and tracking
          - ‚úÖ **Application Performance Auditing**: Lighthouse integration
          - ‚úÖ **Regression Detection**: Automated threshold monitoring
          - ‚úÖ **Performance Alerting**: GitHub issue creation for critical regressions
          - ‚úÖ **Comprehensive Reporting**: Detailed performance dashboard
          
          ---
          *This report was automatically generated by ReadZone Performance Monitoring System*
          EOF
          
          echo "Performance report generated"

      - name: Create GitHub step summary
        run: |
          # Copy report to GitHub step summary
          if [ -f "performance-report.md" ]; then
            cat performance-report.md >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: performance-report.md
          retention-days: 90

      - name: Set workflow status based on performance
        run: |
          BUILD_DURATION="${{ needs.ci-performance-metrics.outputs.build-duration }}"
          PERFORMANCE_SCORE="${{ needs.application-performance-audit.outputs.performance-score }}"
          REGRESSION_DETECTED="${{ needs.performance-regression-detection.outputs.regression-detected }}"
          REGRESSION_SEVERITY="${{ needs.performance-regression-detection.outputs.regression-severity }}"
          
          if [ "$REGRESSION_DETECTED" = "true" ] && [ "$REGRESSION_SEVERITY" = "critical" ]; then
            echo "üö® Critical performance regression detected!"
            echo "Immediate action required to address performance issues."
            exit 1
          elif [ "$BUILD_DURATION" -gt 420 ]; then
            echo "‚ö†Ô∏è Build duration significantly exceeds target (${BUILD_DURATION}s > 420s)"
            echo "Consider optimizing build process."
            exit 1
          elif [ "$PERFORMANCE_SCORE" -lt 70 ] && [ "$PERFORMANCE_SCORE" -gt 0 ]; then
            echo "‚ö†Ô∏è Application performance below acceptable threshold (${PERFORMANCE_SCORE}/100)"
            echo "Performance optimization recommended."
            exit 1
          else
            echo "‚úÖ Performance monitoring completed successfully."
            echo "All performance metrics within acceptable ranges."
            exit 0
          fi